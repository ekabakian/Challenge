{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5791c7f-5497-4fd8-ad4d-9f0a29b4bc9c",
   "metadata": {},
   "source": [
    "## Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12982983-5c55-4057-8a05-4b7e5576328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession \n",
    "import logging\n",
    "import os\n",
    "import datetime, time\n",
    "from pyspark.sql.functions import regexp_replace  \n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, DoubleType, DateType  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aefe0a62-05e3-4d92-8764-03904f907b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('TestEK')\\\n",
    "                .config('spark.master','local[4]')\\\n",
    "                .config('spark.shuffle.sql.partitions',1)\\\n",
    "                .getOrCreate()\n",
    "sqlContext = SparkSession(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16031f6d-7850-4d9f-bee5-c224191d2420",
   "metadata": {},
   "outputs": [],
   "source": [
    "## spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81038f54-c78f-45a6-b884-91829506c726",
   "metadata": {},
   "source": [
    "#### initialize logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab964897-84bb-4ac3-b3fe-251a52d26584",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='Code_EK.log',  \n",
    "                    level=logging.INFO, \n",
    "                    format= '[%(asctime)s] - %(levelname)s - %(message)s',\n",
    "                    datefmt='%H:%M:%S',\n",
    "                    filemode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f095b8-dbc5-49a8-a402-beccadfd9933",
   "metadata": {},
   "source": [
    "## Cargar los datasets utilizando Spark y mantenerlos en formato parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6a972c-40c6-41d8-a978-437bff5c0073",
   "metadata": {},
   "source": [
    "#### Class definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "388d29a6-f125-48e6-b33f-737dcac80104",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFile:\n",
    "    def __init__(self, path, name):\n",
    "        self.path = path\n",
    "        self.name = name\n",
    "        self.dfNews = [] \n",
    "        logging.info(\"*********************************************************\")\n",
    "        logging.info(f\"Iniciando carga de {self.name} con Spark.\")\n",
    "\n",
    "    def readNews(self):\n",
    "        self.dfNews = spark.read.csv(self.path+self.name+\".csv\", sep = ',', header = True, inferSchema = True)\n",
    "        logging.info(f\"El archivo nuevo tiene {self.dfNews.count()} registros.\") \n",
    "\n",
    "    def updateSchema(self):\n",
    "        pass\n",
    "\n",
    "    def initialLoad(self):\n",
    "        self.dfNews.write.parquet(\"parquet/\"+self.name, mode='overwrite')\n",
    "        logging.info(f\"Carga incial completada. Se cargaron {self.dfNews.count()} registros.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d1804e-cd2f-4ab2-8ba4-cae82c1bde60",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### CountryWiseLatest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ae93f23-da42-4657-8f6f-94343d4cf544",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountryWiseLatest(MyFile):\n",
    "    def __init__(self,path, name):\n",
    "        super().__init__(path, name)\n",
    "\n",
    "    def updateSchema(self):\n",
    "        self.dfNews = self.dfNews.withColumnsRenamed({\"Country/Region\":\"Country\",\n",
    "                            \"New cases\":\"NewCases\",\n",
    "                            \"New deaths\":\"NewDeaths\",\n",
    "                            \"New recovered\":\"NewRecovered\",\n",
    "                            \"Deaths / 100 Cases\":\"Deaths100Cases\",\n",
    "                            \"Recovered / 100 Cases\":\"Recovered100Cases\",\n",
    "                            \"Deaths / 100 Recovered\":\"Deaths100Recovered\",\n",
    "                            \"Confirmed last week\":\"ConfirmedLastWeek\",\n",
    "                            \"1 week change\":\"weekChange\",\n",
    "                            \"1 week % increase\":\"weekIncrease\",\n",
    "                            \"WHO Region\":\"WHORegion\"\n",
    "                           })\n",
    "        \n",
    "    def incrementalLoad(self):\n",
    "        df = spark.read.parquet(\"parquet/\"+self.name, sep = ',', header = True, inferSchema = True)\n",
    "        logging.info(f\"El archivo destino tiene {df.count()} registros.\") \n",
    "        \n",
    "        dfToUpdate = self.dfNews.join(df,(df.Country == self.dfNews.Country) &\n",
    "                             (df.WHORegion == self.dfNews.WHORegion) &\n",
    "                             ((df.Confirmed!=self.dfNews.Confirmed) |\n",
    "                             (df.Deaths!=self.dfNews.Deaths) |\n",
    "                             (df.Recovered!=self.dfNews.Recovered) |\n",
    "                             (df.Active!=self.dfNews.Active) | \n",
    "                             (df.NewCases!=self.dfNews.NewCases) |\n",
    "                             (df.NewDeaths!=self.dfNews.NewDeaths) |\n",
    "                             (df.NewRecovered!=self.dfNews.NewRecovered) |\n",
    "                             (df.Deaths100Cases!=self.dfNews.Deaths100Cases) |\n",
    "                             (df.Recovered100Cases!=self.dfNews.Recovered100Cases) |\n",
    "                             (df.Deaths100Recovered!=self.dfNews.Deaths100Recovered) |\n",
    "                             (df.ConfirmedLastWeek !=self.dfNews.ConfirmedLastWeek ) |\n",
    "                             (df.weekChange!=self.dfNews.weekChange) |\n",
    "                             (df.weekIncrease!=self.dfNews.weekIncrease))                        \n",
    "                            , \"inner\").select(self.dfNews.Country,self.dfNews.Confirmed,self.dfNews.Deaths,self.dfNews.Recovered,self.dfNews.Active,\n",
    "                                              self.dfNews.NewCases,self.dfNews.NewDeaths,self.dfNews.NewRecovered,self.dfNews.Deaths100Cases,\n",
    "                                              self.dfNews.Recovered100Cases,self.dfNews.Deaths100Recovered,self.dfNews.ConfirmedLastWeek,\n",
    "                                              self.dfNews.weekChange,self.dfNews.weekIncrease,self.dfNews.WHORegion)\n",
    "\n",
    "        df_u = dfToUpdate.count()\n",
    "        \n",
    "        df = df.join(dfToUpdate,(df.Country == self.dfNews.Country) & (df.WHORegion == self.dfNews.WHORegion), \"leftanti\")\n",
    "        \n",
    "        dfToInsert = self.dfNews.join(df,(df.Country == self.dfNews.Country) & (df.WHORegion == self.dfNews.WHORegion), \"leftanti\")\n",
    "\n",
    "        df_i = dfToInsert.count()-dfToUpdate.count()\n",
    "\n",
    "        df = df.union(dfToInsert)\n",
    "\n",
    "        df.write.parquet(\"parquet/\"+self.name,mode='overwrite')\n",
    "        logging.info(f\"Se actualizaron {df_u} registros y se insertaron {df_i} nuevos.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984af08e-9ccf-4db1-b669-b52d8f632564",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### FullGrouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1307041-aa7b-41c5-b11e-a36a3b2caf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullGrouped(MyFile):\n",
    "    def __init__(self,path, name):\n",
    "        super().__init__(path, name)\n",
    "\n",
    "    def updateSchema(self):\n",
    "        self.dfNews = self.dfNews.withColumnsRenamed({\"Country/Region\":\"Country\",\n",
    "                            \"New cases\":\"NewCases\",\n",
    "                            \"New deaths\":\"NewDeaths\",\n",
    "                            \"New recovered\":\"NewRecovered\", \n",
    "                            \"WHO Region\":\"WHORegion\"\n",
    "                           })\n",
    "        \n",
    "    def incrementalLoad(self):\n",
    "        df = spark.read.parquet(\"parquet/\"+self.name, sep = ',', header = True, inferSchema = True)\n",
    "        logging.info(f\"El archivo destino tiene {df.count()} registros.\") \n",
    "        \n",
    "        dfToUpdate = self.dfNews.join(df,(df.Date == self.dfNews.Date) &\n",
    "                             (df.Country == self.dfNews.Country) &\n",
    "                             (df.WHORegion == self.dfNews.WHORegion) &\n",
    "                             ((df.Confirmed!=self.dfNews.Confirmed) |\n",
    "                             (df.Deaths!=self.dfNews.Deaths) |\n",
    "                             (df.Recovered!=self.dfNews.Recovered) |\n",
    "                             (df.Active!=self.dfNews.Active) | \n",
    "                             (df.NewCases!=self.dfNews.NewCases) |\n",
    "                             (df.NewDeaths!=self.dfNews.NewDeaths) |\n",
    "                             (df.NewRecovered!=self.dfNews.NewRecovered))                        \n",
    "                            , \"inner\").select(self.dfNews.Date,self.dfNews.Country,self.dfNews.Confirmed,self.dfNews.Deaths,\n",
    "                                              self.dfNews.Recovered,self.dfNews.Active,self.dfNews.NewCases,self.dfNews.NewDeaths,\n",
    "                                              self.dfNews.NewRecovered,self.dfNews.WHORegion)\n",
    "\n",
    "        df_u = dfToUpdate.count()\n",
    "        \n",
    "        df = df.join(dfToUpdate,(df.Date == self.dfNews.Date) & \n",
    "                                (df.Country == self.dfNews.Country) & \n",
    "                                (df.WHORegion == self.dfNews.WHORegion), \"leftanti\")\n",
    "        \n",
    "        dfToInsert = self.dfNews.join(df,(df.Date == self.dfNews.Date) & \n",
    "                                         (df.Country == self.dfNews.Country) & \n",
    "                                         (df.WHORegion == self.dfNews.WHORegion), \"leftanti\")\n",
    "\n",
    "        df_i = dfToInsert.count()-dfToUpdate.count()\n",
    "\n",
    "        df = df.union(dfToInsert)\n",
    "\n",
    "        df.write.parquet(\"parquet/\"+self.name,mode='overwrite')\n",
    "        logging.info(f\"Se actualizaron {df_u} registros y se insertaron {df_i} nuevos.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385feae4-538b-4018-980b-364c124820d4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### Covid19CleanComplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bab76c8-3cfc-466f-840f-a5ae7851140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Covid19CleanComplete(MyFile):\n",
    "    def __init__(self,path, name):\n",
    "        super().__init__(path, name)\n",
    "\n",
    "    def updateSchema(self):\n",
    "        self.dfNews = self.dfNews.withColumnsRenamed({\"Province/State\":\"State\",\n",
    "                            \"Country/Region\":\"Country\", \n",
    "                            \"WHO Region\":\"WHORegion\"\n",
    "                           }) \n",
    "        self.dfNews = self.dfNews.fillna(\"unknown\")\n",
    "        \n",
    "    def incrementalLoad(self):\n",
    "        df = spark.read.parquet(\"parquet/\"+self.name, sep = ',', header = True, inferSchema = True)\n",
    "        logging.info(f\"El archivo destino tiene {df.count()} registros.\") \n",
    "\n",
    "        \n",
    "        dfToUpdate = self.dfNews.join(df,(df.Date == self.dfNews.Date) &\n",
    "                             (df.WHORegion == self.dfNews.WHORegion) &\n",
    "                             (df.Country == self.dfNews.Country) & \n",
    "                             (df.State == self.dfNews.State) & \n",
    "                             ((df.Lat!=self.dfNews.Lat) |\n",
    "                             (df.Long!=self.dfNews.Long) | \n",
    "                             (df.Confirmed!=self.dfNews.Confirmed) | \n",
    "                             (df.Deaths!=self.dfNews.Deaths) |\n",
    "                             (df.Recovered!=self.dfNews.Recovered) |\n",
    "                             (df.Active!=self.dfNews.Active))                       \n",
    "                            , \"inner\").select(self.dfNews.State,self.dfNews.Country,self.dfNews.Lat,self.dfNews.Long,self.dfNews.Date,\n",
    "                                              self.dfNews.Confirmed,self.dfNews.Deaths,self.dfNews.Recovered,self.dfNews.Active,self.dfNews.WHORegion)\n",
    "\n",
    "        df_u = dfToUpdate.count()\n",
    "        \n",
    "        df = df.join(dfToUpdate,(df.Date == self.dfNews.Date) &\n",
    "                                (df.WHORegion == self.dfNews.WHORegion) & \n",
    "                                (df.Country == self.dfNews.Country) & \n",
    "                                (df.State == self.dfNews.State), \"leftanti\")\n",
    "        \n",
    "        dfToInsert = self.dfNews.join(df,(df.Date == self.dfNews.Date) &\n",
    "                                         (df.WHORegion == self.dfNews.WHORegion) & \n",
    "                                         (df.Country == self.dfNews.Country) & \n",
    "                                         (df.State == self.dfNews.State), \"leftanti\")\n",
    "\n",
    "        df_i = dfToInsert.count()-dfToUpdate.count()\n",
    "\n",
    "        df = df.union(dfToInsert)\n",
    "\n",
    "        df.write.parquet(\"parquet/\"+self.name,mode='overwrite')\n",
    "        logging.info(f\"Se actualizaron {df_u} registros y se insertaron {df_i} nuevos.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a170b266-1098-4d7b-b4d5-db944abe9765",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### WorldometerData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6a7a07b-9f77-450e-9c7b-c2683e924735",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorldometerData(MyFile):\n",
    "    def __init__(self,path, name):\n",
    "        super().__init__(path, name) \n",
    "\n",
    "    def updateSchema(self):\n",
    "        self.dfNews = self.dfNews.withColumnsRenamed({\"Country/Region\":\"Country\",\n",
    "                            \"Serious,Critical\":\"Serious\",\n",
    "                            \"Tot Cases/1M pop\":\"TotCases1MPop\",\n",
    "                            \"Deaths/1M pop\":\"Deaths1MPop\",\n",
    "                            \"Tests/1M pop\":\"Tests1MPop\",\n",
    "                            \"WHO Region\":\"WHORegion\"\n",
    "                           })  \n",
    "        self.dfNews = self.dfNews.fillna(\"unknown\")\n",
    "        \n",
    "    def incrementalLoad(self):\n",
    "        df = spark.read.parquet(\"parquet/\"+self.name, sep = ',', header = True, inferSchema = True)\n",
    "        logging.info(f\"El archivo destino tiene {df.count()} registros.\") \n",
    "        \n",
    "        dfToUpdate = self.dfNews.join(df,(df.WHORegion == self.dfNews.WHORegion) & \n",
    "\t\t\t\t\t\t\t (df.Continent == self.dfNews.Continent) &  \n",
    "\t\t\t\t\t\t\t (df.Country == self.dfNews.Country) & \n",
    "                             ((df.Population!=self.dfNews.Population) |\n",
    "                             (df.TotalCases!=self.dfNews.TotalCases) | \n",
    "                             (df.NewCases!=self.dfNews.NewCases) |\n",
    "                             (df.TotalDeaths!=self.dfNews.TotalDeaths) |\n",
    "                             (df.NewDeaths!=self.dfNews.NewDeaths) |\n",
    "                             (df.TotalRecovered!=self.dfNews.TotalRecovered) |\n",
    "                             (df.NewRecovered!=self.dfNews.NewRecovered) |\n",
    "                             (df.ActiveCases!=self.dfNews.ActiveCases) |\n",
    "                             (df.Serious!=self.dfNews.Serious) |\n",
    "                             (df.TotCases1MPop!=self.dfNews.TotCases1MPop) |\n",
    "                             (df.Deaths1MPop!=self.dfNews.Deaths1MPop) |\n",
    "                             (df.TotalTests!=self.dfNews.TotalTests) |\n",
    "                             (df.Tests1MPop!=self.dfNews.Tests1MPop))                \n",
    "                            , \"inner\").select(self.dfNews.Country,self.dfNews.Continent,self.dfNews.Population,self.dfNews.TotalCases,\n",
    "                                              self.dfNews.NewCases,self.dfNews.TotalDeaths,self.dfNews.NewDeaths,self.dfNews.TotalRecovered,\n",
    "                                              self.dfNews.NewRecovered,self.dfNews.ActiveCases,self.dfNews.Serious,self.dfNews.TotCases1MPop,\n",
    "                                              self.dfNews.Deaths1MPop,self.dfNews.TotalTests,self.dfNews.Tests1MPop,self.dfNews.WHORegion)\n",
    "\n",
    "        df_u = dfToUpdate.count()\n",
    "        \n",
    "        df = df.join(dfToUpdate,(df.WHORegion == self.dfNews.WHORegion) & \n",
    "\t\t\t\t\t\t\t (df.Continent == self.dfNews.Continent) &  \n",
    "\t\t\t\t\t\t\t (df.Country == self.dfNews.Country), \"leftanti\")\n",
    "        \n",
    "        dfToInsert = self.dfNews.join(df,(df.WHORegion == self.dfNews.WHORegion) & \n",
    "\t\t\t\t\t\t\t (df.Continent == self.dfNews.Continent) &  \n",
    "\t\t\t\t\t\t\t (df.Country == self.dfNews.Country), \"leftanti\")\n",
    "\n",
    "        df_i = dfToInsert.count()-dfToUpdate.count()\n",
    "\n",
    "        df = df.union(dfToInsert)\n",
    "\n",
    "        df.write.parquet(\"parquet/\"+self.name,mode='overwrite')\n",
    "        logging.info(f\"Se actualizaron {df_u} registros y se insertaron {df_i} nuevos.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4feed8-9099-408c-9548-f7bed19c7d23",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### DayWise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c1b1dda-7528-4769-ad0c-b491e08e4e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DayWise(MyFile):\n",
    "    def __init__(self,path, name):\n",
    "        super().__init__(path, name)\n",
    "\n",
    "    def updateSchema(self):\n",
    "        self.dfNews = self.dfNews.withColumnsRenamed({\"New cases\":\"NewCases\",\n",
    "                            \"New deaths\":\"NewDeaths\",\n",
    "                            \"New recovered\":\"NewRecovered\",\n",
    "                            \"Deaths / 100 Cases\":\"Deaths100Cases\",\n",
    "                            \"Recovered / 100 Cases\":\"Recovered100Cases\",\n",
    "                            \"Deaths / 100 Recovered\":\"Deaths100Recovered\",\n",
    "                            \"No. of countries\":\"CountriesNr\"\n",
    "                           })  \n",
    "        \n",
    "    def incrementalLoad(self):\n",
    "        df = spark.read.parquet(\"parquet/\"+self.name, sep = ',', header = True, inferSchema = True)\n",
    "        logging.info(f\"El archivo destino tiene {df.count()} registros.\") \n",
    "        \n",
    "        dfToUpdate = self.dfNews.join(df,(df.Date == self.dfNews.Date) & \n",
    "                             ((df.Confirmed!=self.dfNews.Confirmed) |\n",
    "                             (df.Deaths!=self.dfNews.Deaths) |\n",
    "                             (df.Recovered!=self.dfNews.Recovered) |\n",
    "                             (df.Active!=self.dfNews.Active) | \n",
    "                             (df.NewCases!=self.dfNews.NewCases) |\n",
    "                             (df.NewDeaths!=self.dfNews.NewDeaths) |\n",
    "                             (df.NewRecovered!=self.dfNews.NewRecovered) |\n",
    "                             (df.Deaths100Cases!=self.dfNews.Deaths100Cases) |\n",
    "                             (df.Recovered100Cases!=self.dfNews.Recovered100Cases) |\n",
    "                             (df.Deaths100Recovered!=self.dfNews.Deaths100Recovered) |\n",
    "                             (df.CountriesNr!=self.dfNews.CountriesNr))                \n",
    "                            , \"inner\").select(self.dfNews.Date,self.dfNews.Confirmed,self.dfNews.Deaths,self.dfNews.Recovered,self.dfNews.Active, \n",
    "                                              self.dfNews.NewCases,self.dfNews.NewDeaths,self.dfNews.NewRecovered,self.dfNews.Deaths100Cases,\n",
    "                                              self.dfNews.Recovered100Cases,self.dfNews.Deaths100Recovered,self.dfNews.CountriesNr)\n",
    "\n",
    "        df_u = dfToUpdate.count()\n",
    "        \n",
    "        df = df.join(dfToUpdate,(df.Date == self.dfNews.Date), \"leftanti\")\n",
    "        \n",
    "        dfToInsert = self.dfNews.join(df,(df.Date == self.dfNews.Date), \"leftanti\")\n",
    "\n",
    "        df_i = dfToInsert.count()-dfToUpdate.count()\n",
    "\n",
    "        df = df.union(dfToInsert)\n",
    "\n",
    "        df.write.parquet(\"parquet/\"+self.name,mode='overwrite')\n",
    "        logging.info(f\"Se actualizaron {df_u} registros y se insertaron {df_i} nuevos.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2513de-9fa5-4a86-8860-07a490482639",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### UsaCountyWise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "911deed2-9016-4dc2-807c-db81749281d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UsaCountyWise(MyFile):\n",
    "    def __init__(self,path, name):\n",
    "        super().__init__(path, name)\n",
    "     \n",
    "    def updateSchema(self):\n",
    "        self.dfNews = self.dfNews.withColumnsRenamed({\"Province_State\":\"State\",\n",
    "                            \"Country_Region\":\"Country\",\n",
    "                            \"Long_\":\"Long\",\n",
    "                            \"Combined_Key\":\"CombinedKey\"\n",
    "                           })  \n",
    "        self.dfNews = self.dfNews.fillna(\"unknown\")\n",
    "        \n",
    "    def incrementalLoad(self):\n",
    "        df = spark.read.parquet(\"parquet/\"+self.name, sep = ',', header = True, inferSchema = True)\n",
    "        logging.info(f\"El archivo destino tiene {df.count()} registros.\") \n",
    "        \n",
    "        dfToUpdate = self.dfNews.join(df,(df.Date == self.dfNews.Date) & \n",
    "                             (df.State == self.dfNews.State) & \n",
    "                             (df.Country == self.dfNews.Country) & \n",
    "                             (df.Admin2 == self.dfNews.Admin2) &\n",
    "                             ((df.UID!=self.dfNews.UID) |\n",
    "                             (df.iso2!=self.dfNews.iso2) |\n",
    "                             (df.iso3!=self.dfNews.iso3) |\n",
    "                             (df.code3!=self.dfNews.code3) | \n",
    "                             (df.FIPS!=self.dfNews.FIPS) | \n",
    "                             (df.Lat!=self.dfNews.Lat) |\n",
    "                             (df.Long!=self.dfNews.Long) |\n",
    "                             (df.CombinedKey!=self.dfNews.CombinedKey) | \n",
    "                             (df.Confirmed!=self.dfNews.Confirmed) |\n",
    "                             (df.Deaths!=self.dfNews.Deaths))                \n",
    "                            , \"inner\").select(self.dfNews.UID,self.dfNews.iso2, self.dfNews.iso3,self.dfNews.code3,self.dfNews.FIPS,\n",
    "                                              self.dfNews.Admin2,self.dfNews.State,self.dfNews.Country,self.dfNews.Lat,self.dfNews.Long,\n",
    "                                              self.dfNews.CombinedKey,self.dfNews.Date,self.dfNews.CombinedKey,self.dfNews.Confirmed,self.dfNews.Deaths)\n",
    "\n",
    "        df_u = dfToUpdate.count()\n",
    "        \n",
    "        df = df.join(dfToUpdate,(df.Date == self.dfNews.Date) & \n",
    "                             (df.State == self.dfNews.State) & \n",
    "                             (df.Country == self.dfNews.Country) &\n",
    "                             (df.Admin2 == self.dfNews.Admin2) , \"leftanti\")\n",
    "        \n",
    "        dfToInsert = self.dfNews.join(df,(df.Date == self.dfNews.Date) & \n",
    "                             (df.State == self.dfNews.State) & \n",
    "                             (df.Country == self.dfNews.Country) &\n",
    "                             (df.Admin2 == self.dfNews.Admin2), \"leftanti\")\n",
    "\n",
    "        df_i = dfToInsert.count()-dfToUpdate.count()\n",
    "\n",
    "        df = df.union(dfToInsert)\n",
    "\n",
    "        df.write.parquet(\"parquet/\"+self.name,mode='overwrite')\n",
    "        logging.info(f\"Se actualizaron {df_u} registros y se insertaron {df_i} nuevos.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a00131-6665-42ac-86c0-3ea3acc16f2c",
   "metadata": {},
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2316df3c-4c56-4e4d-80b9-6e6e3f15bc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./archive/\"\n",
    "files = os.listdir(path)\n",
    "files = list(map(lambda x: x[0:x.find(\".\")] ,filter(lambda f: f.endswith('.csv'), files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04444c70-bd1c-484c-b3e6-6be1d0dcd30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country_wise_latest\n",
      "covid_19_clean_complete\n",
      "day_wise\n",
      "full_grouped\n",
      "worldometer_data\n"
     ]
    }
   ],
   "source": [
    "for x in files:\n",
    "    if x == \"country_wise_latest\":\n",
    "        file = CountryWiseLatest(path,x)\n",
    "    elif x == \"full_grouped\":\n",
    "        file = FullGrouped(path,x)\n",
    "    elif x == \"covid_19_clean_complete\":\n",
    "        file = Covid19CleanComplete(path,x)\n",
    "    elif x == \"day_wise\":\n",
    "        file = DayWise(path,x)\n",
    "    elif x == \"usa_county_wise\":\n",
    "        continue\n",
    "    elif x == \"worldometer_data\":\n",
    "        file = WorldometerData(path,x)\n",
    "    file.readNews()\n",
    "    file.updateSchema()\n",
    "    if x in os.listdir(\"./parquet\"):\n",
    "        file.incrementalLoad()\n",
    "    else:\n",
    "        file.initialLoad()\n",
    "    print(x)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
